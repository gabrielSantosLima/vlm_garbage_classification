{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9db28f7",
   "metadata": {},
   "source": [
    "# Testing Gemma3 270m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea549644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import json\n",
    "import kagglehub\n",
    "import keras_hub\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import trange\n",
    "from scripts.dataset import Dataset\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "sns.set_theme(palette=sns.color_palette(\"rocket_r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(\"kaggle.json\"), \"Kaggle credentials are required to run this project.\"\n",
    "\n",
    "credentials = None\n",
    "with open(\"kaggle.json\") as kaggle_json:\n",
    "    credentials = json.load(kaggle_json)\n",
    "\n",
    "print(f\"Using credentials = {credentials}\")\n",
    "os.environ['KAGGLE_USERNAME'] = credentials[\"username\"]\n",
    "os.environ['KAGGLE_KEY'] = credentials[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "\n",
    "DATASET_PATH = os.path.join(CURRENT_PATH, \"dataset.csv\")\n",
    "EXPERIMENT_PATH = os.path.join(CURRENT_PATH, '__experiments__')\n",
    "EXPERIMENT_NAME = \"llm_model\"\n",
    "\n",
    "# Trainig Parameters\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "# LLM Prediction (seconds)\n",
    "DELAY_BEFORE_NEXT = 0.5\n",
    "CHECKPOINT_PATH = os.path.join(CURRENT_PATH, \"report-gemma3-270m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(DATASET_PATH), \"Dataset not exists. Please run the previous notebook.\"\n",
    "test_dataset = Dataset(dataset_path=DATASET_PATH, dataset_mode=\"csv\", partition_name=\"test\", preload=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_numpy(dataset: Dataset):\n",
    "    n = len(dataset)\n",
    "    iterator = iter(dataset)\n",
    "    x = np.ndarray(shape=(n, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL), dtype=np.float64)\n",
    "    y = np.ndarray(shape=(n,), dtype=np.int8)\n",
    "    names = np.ndarray(shape=(n,), dtype=object)\n",
    "    for index in trange(n):\n",
    "        xi, yi, image_name = next(iterator)\n",
    "        xi = cv.resize(xi, [IMAGE_WIDTH, IMAGE_HEIGHT])\n",
    "        x[index] = xi\n",
    "        y[index] = yi\n",
    "        names[index] = image_name\n",
    "    return x, y, names\n",
    "\n",
    "x_test, y_test, names = as_numpy(test_dataset)\n",
    "print(f\"Image Shape = {x_test[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38dd93",
   "metadata": {},
   "source": [
    "## Preparing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.model_download(\"keras/gemma3/keras/gemma3_270m\")\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b666a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras_hub.models.Gemma3CausalLM.from_preset(path)\n",
    "# model.generate(\"Keras is a\", max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.ndarray(shape=y_test.shape, dtype=np.int8)\n",
    "\n",
    "checkpoint = {\n",
    "    \"image_name\": [],\n",
    "    \"y_true\": [],\n",
    "    \"y_pred\": [],\n",
    "    \"y_true_as_str\": [],\n",
    "    \"y_pred_as_str\": [],\n",
    "}\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for index in trange(len(x_test)):\n",
    "    xi = x_test[index]\n",
    "    yi = y_test[index]\n",
    "    name = names[index]\n",
    "    \n",
    "    # Prediction...\n",
    "    prediction = 0\n",
    "    \n",
    "    # Saving results\n",
    "    y_pred[index] = prediction\n",
    "    \n",
    "    # Saving checkpoint\n",
    "    checkpoint[\"image_name\"].append(name)\n",
    "    checkpoint[\"y_true\"].append(yi)\n",
    "    checkpoint[\"y_pred\"].append(prediction)\n",
    "    checkpoint[\"y_true_as_str\"].append(test_dataset.decode_label(yi))\n",
    "    checkpoint[\"y_pred_as_str\"].append(test_dataset.decode_label(prediction))\n",
    "    pd.DataFrame.from_dict(checkpoint).to_csv(CHECKPOINT_PATH, sep=\";\", index=False)\n",
    "\n",
    "    # Delay before next prediction\n",
    "    sleep(DELAY_BEFORE_NEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2754c99",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab1047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def plot_metrics(y_true: np.array, y_pred: np.array):\n",
    "    table = PrettyTable([\"Metric\", \"Value (%)\"])\n",
    "    \n",
    "    # Calculating metrics...\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0.0) * 100\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0.0) * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0.0) * 100\n",
    "\n",
    "    table.add_row(['Accuracy', f\"{accuracy:.2f}%\"])\n",
    "    table.add_row(['Precision', f\"{precision:.2f}%\"])\n",
    "    table.add_row(['Recall', f\"{recall:.2f}%\"])\n",
    "    table.add_row(['F1 Score', f\"{f1:.2f}%\"])\n",
    "    \n",
    "    print(table)\n",
    "\n",
    "def plot_cm(y_true: np.array, y_pred: np.array):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    sns.heatmap(cm, annot=True, cmap=sns.color_palette(\"rocket_r\", as_cmap=True), fmt='.0f', yticklabels=test_dataset.classes, xticklabels=test_dataset.classes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f08205",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(y_test, y_pred)\n",
    "plot_cm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af173135",
   "metadata": {},
   "source": [
    "Visualizing some errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54dd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for index in range(len(y_pred)):\n",
    "    true = y_test[index] \n",
    "    pred = y_pred[index]\n",
    "    if true != pred:\n",
    "        errors.append((x_test[index], true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b46504",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 5\n",
    "ncols = 4\n",
    "\n",
    "figure, axis = plt.subplots(nrows, ncols, squeeze=False, figsize=(14,18))\n",
    "figure.suptitle(f\"Errors\")\n",
    "axes = []\n",
    "for ax in axis:\n",
    "    axes += [*ax]\n",
    "for index in range(nrows * ncols):\n",
    "    ax = axes[index]\n",
    "    x, true_class, pred = errors[index]\n",
    "    pred = test_dataset.decode_label(pred)\n",
    "    true_class = test_dataset.decode_label(true_class)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"[y_true={true_class}, y_pred={pred}]\")\n",
    "    ax.imshow(x.astype(np.int64))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
